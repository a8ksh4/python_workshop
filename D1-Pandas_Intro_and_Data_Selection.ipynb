{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section D1 - Pandas Intro and Data Selection\n",
    "**Index**\n",
    "* General Info\n",
    "* Creating a Dataframe\n",
    "* Selecting Columns by Name\n",
    "* Selecting Rows and Columns with loc and iloc\n",
    "* Using .iterrows\n",
    "* Selecting Rows with a mask\n",
    "* Selecting Rows with .query\n",
    "\n",
    "## General Info\n",
    "Pandas has two types of objects, **DataFrames** and **Series**.  A dataframe has rows and columns, like a spreadsheet - two dimensional.  A single row or column from a dataframe is a Series.  If we select a single column from a DataFrame, we get a series, a single dimensional object, and a series can be inserted into a df column. \n",
    "\n",
    "By convention, we'll import pandas as \"pd\" to save us some typing.\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    " It's also common to call a single dataframe that we're working on \"df\", but it's a good idea to use a longer more descriptive name for complex tasks.\n",
    "\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "\n",
    "There is functionality built into pd, as well as the dataframe and series objects that we create that we will use to manipulate the dataframe and series.  For example, we use these DataFrame functions a lot to view our data:\n",
    "\n",
    "    df.info()  # show a summary of columns and data types in the dataframe. \n",
    "    df.head()  # show the top few rows of the dataframe.\n",
    "    df.tail()  # few bottom rows\n",
    "    df.describe()\n",
    "    ...and more\n",
    "\n",
    "And there are functions we call from pd to manipulate the dataframes:\n",
    "\n",
    "    big_df = pd.concat(a_list_of_small_dataframes)  # concatenate dataframes together\n",
    "    ...and more\n",
    "\n",
    "**A note regarding inplace operations** - Many pandas functions take an argument \"inplace=True/False\".  Setting it to true means that the change will be made on the existing dataframe that your variable points to and the function will return None.  Setting it to false (or omiting the option) means that the function will return a modified copy of the dataframe that you need to assign to your variable to see the changes.  These are roughly equivelant:\n",
    "    \n",
    "    df = df.foo()  # inplace=False is default\n",
    "    df.foo(inplace=True)\n",
    "\n",
    "\n",
    "## Creating a Dataframe\n",
    "We can create an empty dataframe:\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "But generally (or always) we'll want to load some data to make a dataframe. Common ways to do this follow. Reference the documentation to see optional arguments to use, like \"skip_rows\" to skip padding rows at the top of an excel or csv file, or use_cols to only import specific columns. \n",
    "\n",
    "**Excel Files** - https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n",
    "\n",
    "    df = pd.read_excel(file_name, ... engine ...)\n",
    "\n",
    "**CSV Files or dat Files** - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "You may need to set the delimeter for some csv files. \n",
    "\n",
    "    df = pd.read_csv(file_name, ...)\n",
    "    df = pd.read_table(file_name, ...)\n",
    "\n",
    "**json Data** - https://pandas.pydata.org/docs/reference/api/pandas.read_json.html\n",
    "Useful for data loaded from the web.  This is what we use in the D1-Pandas_Example notebook.\n",
    "\n",
    "    df = pd.read_json(json_data, ...)\n",
    "\n",
    "The json data would need to be structured as a dictionary of lists or a list of dictionaries, as described in the next two examples!\n",
    "\n",
    "**Dictionary of Lists to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**list of dictionaries to DataFrame**\n",
    "Same idea as above, but slightly different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},\n",
    "    {'Name': 'Bob', 'Age': 30, 'City': 'Los Angeles'},\n",
    "    {'Name': 'Charlie', 'Age': 35, 'City': 'Chicago'}\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*\n",
    "\n",
    "In the following code cell, use these functions to look at information about the dataframe:\n",
    "\n",
    "    .info(), .describe(), and .head() \n",
    "\n",
    "And print thef following properties of the dataframe, like: `df.shape`\n",
    "\n",
    "    .columns, .size, .shape\n",
    "\n",
    "* What data type is each of the columns?\n",
    "* How many rows and columns are there?\n",
    "* What's the relationship between shape and size?\n",
    "* Use a list comprehension to overwrite df.columns and make the comlumn names upper case.  `df.columns = [... ... df.comumns]`\n",
    "\n",
    "Scroll through the DataFrame documentation to get an idea of what methods are built into it: https://pandas.pydata.org/pandas-docs/stable/reference/frame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *We'll use this flowers dataframe for a few exercises below, so make sure to run this cell before continuing.*\n",
    "flowers = pd.read_csv(\"https://raw.githubusercontent.com/a8ksh4/python_workshop/main/SAMPLE_DATA/iris.csv\")\n",
    "# You can also try saving iris.csv in the directory with your notebook and opening it from a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.  You can re-run the above cell if you mess up your dataframe.\n",
    "# print(flowers....)\n",
    "flowers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Index and Columns\n",
    "**df.index** and **df.columns** are identifiers for rows and columns of a dataframe.  They can both be numeric or descriptive.  It's common to have descriptive column names and an index matching the row numbers, but we can re-assign the index to a column, like a primary-key, before we join two dataframes or perform similar operations between dataframes that reference the index. As we'll look at below, we can also use .iloc to access and modify rows based on their index and columns, so we should know how to set them.\n",
    "set to a column that has a sort of primary key for each row of the data. \n",
    "Let's create a new dataframe for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.DataFrame({'fruit': ['apple', 'banana', 'cherry', 'date'],\n",
    "                   'color': ['red', 'yellow', 'red', 'brown'],\n",
    "                   'weight kg': [0.2, 0.3, 0.05, 0.1]})\n",
    "print(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the index matches the row numbers.  Let's do a few manipulations of the index to see how that works:\n",
    "* Change the index to the fruit column\n",
    "* Modify the index after changing it. \n",
    "* Reset the index back to numeric\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html#pandas.DataFrame.set_index\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = fruits.set_index('fruit')\n",
    "print(fruits)\n",
    "fruits.index = fruits.index.str.capitalize()\n",
    "print(fruits)\n",
    "fruits = fruits.reset_index()\n",
    "print(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do something similar with the columns:\n",
    "* We'll capitalize the columns\n",
    "* and replace any spaces in the column names with an underscore character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.columns = fruits.columns.str.capitalize()\n",
    "fruits.columns = fruits.columns.str.replace(' ', '_')\n",
    "print(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Columns by name:\n",
    "We can select a single column by passing it's name in brackets, like: `df['column_name']`\n",
    "\n",
    "And we can select multiple columns by passing a list of column names in nested brackets: `df[['column1', 'column2', ...]]`\n",
    "\n",
    "This is a bit like string or list slicing, but using names or lists of names to take a selection of the available columns.\n",
    "\n",
    "We can use this to both get values from columns or to assign values directly into one or more columns, or to create new columns of some name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single column is a series object, so sepal_lenghts is a series.\n",
    "sls = flowers['sepal_length']\n",
    "print('Some of the sepal lenghths are:\\n', sls)\n",
    "print('All the lenghts are:\\n', list(sls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*\n",
    "Just like we did for the dataframe above, let's explore this \"sls\" series object.\n",
    "\n",
    "* Use the `.info(), .shape, .size` properties to learn about the object. \n",
    "* And Let's try some more interesting functions built into series objects: `.sum(), .value_counts(), .mean()`\n",
    "* Check if the series is greater than 3.  What is returned?  This list of True/False values is important for a future concept, \"masks\", for selecting rows.\n",
    "* Scroll through some of the methods listed in the series documentation here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and manipulating columns of data:\n",
    "We can perform mathematical operations on columns of data and put the result into a new or overwrite an existing column.  For example, if we want to add a column with units inches instead of cm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers['sepal_length_inches'] = flowers['sepal_length'] * 0.393701\n",
    "\n",
    "length_columns = sorted([c for c in flowers.columns if 'length' in c])\n",
    "print('length comparison:\\n', flowers[length_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you perform operations on a column, like multiplying the 'sepal_length' column by 0.393, that operation is broadcast across all rows in the column.  \n",
    "\n",
    "And when we perform operation aginst two columns, each row in the columns is matched with the same index row in the other column for the operation, as with the width_differenc calculation below.\n",
    "\n",
    "We can also select multiple columns py passing the columns in [], like: `df[['petal_length', 'petal_width']]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers['width_difference'] = (flowers['sepal_width'] - flowers['petal_width']).abs()\n",
    "\n",
    "# Alternate ways of selecting and printing columns are commented out below:\n",
    "\n",
    "# width_columns = flowers.columns[df.columns.str.contains('width')]\n",
    "# width_columns = ['sepal_width', 'petal_width', 'width_difference']\n",
    "width_columns = sorted([c for c in flowers.columns if 'width' in c])\n",
    "\n",
    "print('Widths:')\n",
    "# print(flowers[['sepal_width', 'petal_width', 'width_difference']])\n",
    "print(flowers[width_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Rows with loc and iloc\n",
    "**.loc** vs **.iloc**\n",
    "* .loc selects rows with particular labels in the series or dataframe index\n",
    "  * https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html\n",
    "* .iloc selects rows at integer locations within the series or dataframe.\n",
    "  * https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', \n",
    "             'Grace', 'Hannah', 'Isaac', 'Jack'],\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
    "    'SSN': ['123-45-6789', '234-56-7890', '345-67-8901', '456-78-9012', \n",
    "            '567-89-0123', '678-90-1234', '789-01-2345', '890-12-3456', \n",
    "            '901-23-4567', '123-45-5789'],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', \n",
    "             'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose'],\n",
    "})\n",
    "df = df.set_index('SSN')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we set the index of our dataframe to the 'SSN' column, we can use loc to print rows with a specific SSN, or lists of SSNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A single row:\\n', \n",
    "      df.loc['345-67-8901'])\n",
    "print('A list of rows by SSN and a slice of columns from Age to City:\\n', \n",
    "      df.loc[['345-67-8901','456-78-9012'], 'Age':'City'])\n",
    "print('A range of rows by SSN:\\n',\n",
    "      df.loc['345-67-8901':'567-89-0123', 'City'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can include a column name to print specific values or to set them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_ssn = '345-67-8901'\n",
    "print(f'{some_ssn} lives in:', df.loc[some_ssn, 'City'])\n",
    "df.loc[some_ssn, 'City'] = 'Saskatoon'\n",
    "print('Or was it:', df.loc[some_ssn, 'City'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*:\n",
    "A few people have moved, please update their addresses:\n",
    "* People with SSNs '678-90-1234' and '789-01-2345'  didn't pay their taxes and are singing the blues in 'Folsom'. \n",
    "* People with SSNs '890-12-3456', '901-23-4567', and '123-45-5789' are retiring and moved to 'Palm Beach'.\n",
    "How would you do each of these one at a time with a loop, or all at once in a single operation?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc selection of rows and columns\n",
    "Rather than selecting by index value with loc, we can use iloc to select by row address, like 0, 1 or 2, a list of addresses, [1, 2, 3], or a range of addresses, [2:6]. And same for the columns returned.  A few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Row 0:\\n', \n",
    "      df.iloc[0])\n",
    "print('\\nRows 2 and 5 and Age column:\\n', \n",
    "      df.iloc[[2,5], 1])\n",
    "print('\\nRows 2:6 and columns 0 and 1 using slices:\\n', \n",
    "      df.iloc[2:7, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Just like with loc, we can assign values to rows and columns selected using .loc, and we can capture those selections in new dataframes as needed. \n",
    "\n",
    "Also notice that the SSN index is shown...   if you do a .reset_index, you'd instead see a new numerical index instead of the SSNs. \n",
    "We'll look more at the index below.\n",
    "\n",
    "#### *Exercise*\n",
    "Studies have shown that older people tend to be more fun than younger people. \n",
    "* Use iloc to creat two new dataframes called 'top_five' and 'bottom_five' from the top and bottom five rows from 'df'.  \n",
    "* Calculate the average age of each group and determine which group is likely to be the most fun!  You can compute the average of a column using .mean()... something like foo['col_name'].mean(). \n",
    "\n",
    "Do the cities that each group of people live in corroborate the results of the study, or is this silly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using .iterrows() to iterate over rows\n",
    ".iterrows() returns an iterator that we can pair with a for loop to look at each row one at a time.  This isn't in the spirit of pandas, which would prefer that we do something to all of the rows at the same time, but it can be very useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_index, row_vals in df.iterrows():\n",
    "    # print out the name, city, and age of the person in this row:\n",
    "    # print(row[1]['Name'], 'lives in', row[1]['City'], 'and is', row[1]['Age'], 'years old.')\n",
    "    # the [1] is \n",
    "    print('Row_index:', row_index)\n",
    "    print(row_vals['Name'], 'lives in', row_vals['City'], 'and is', row_vals['Age'], 'years old.')\n",
    "    \n",
    "    if row_vals['Name'].startswith('A'):\n",
    "        df.loc[row_index, 'Name'] = df.loc[row_index, 'Name'] + ' was here'\n",
    "        \n",
    "    print('We can use loc to get the name from the same row:', df.loc[row_index, 'Name'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*\n",
    "Use .reset_index() on the df and then iterrows again to see what is changed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting rows with a mask\n",
    "A mask is a way to say \"give me the rows where this condition is true.\"  In pandas, you create the mask by writing a conditional statement resulting in a list of true/false values.  Each true/false corresponds with a row in the dataframe. Applying the mask gives you only the rows with a corresponding true value.\n",
    "\n",
    "We'll look at conditional statements and then a mask example.\n",
    "\n",
    "### Conditional statements\n",
    "Here are a few examples of conditional statements:\n",
    "* Their age is greater than 30:\n",
    "  * `df['Age'] > 30`\n",
    "* Their name contains the letter 'a' and they are older than 40:\n",
    "  * `df['Name'].str.lower().str.contains('a') & (df['Age'] > 40)`\n",
    "* They are older than 50 or younger than 30:\n",
    "  * `(df['Age'] > 50) | (df['Age'] <= 30)`\n",
    "\n",
    "Note that rather than \"and\" and \"or\" in regular python code, we use \"&\" and \"|\" when comparing pandas series.  These are python bitwise operators. \n",
    "\n",
    "* Bitwise And: `a & b`\n",
    "* Bitwise Exclusive Or: `a ^ b`\n",
    "* Bitwise Inversion (not): `~ a`\n",
    "* Bitwise Or: `a | b`\n",
    "\n",
    "And when using & and |, we need to put parenthesees around the other expressions to make sure they are evaluated before the bitwise operators.  \n",
    "* This will error:\n",
    "  * `df['Age'] > 50 | df['Age'] <= 30`\n",
    "* This is correct:\n",
    "  * `(df['Age'] > 50) | (df['Age'] <= 30)`\n",
    "\n",
    "https://introcs.cs.princeton.edu/python/appendix_precedence/#:~:text=Order%20of%20Evaluation,the%20and%20or%20or%20operators.\n",
    "https://docs.python.org/3/library/operator.html#mapping-operators-to-functions\n",
    "\n",
    "### Example use of a mask to select some rows:\n",
    "Let's select all people/rows from our dataframe where their age is > 45:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_over_45 = df['Age'] > 45\n",
    "# mask_under_eq_45 = ~mask_over_45  # example of inverting/negating a mask\n",
    "# mask_under_eq_45 = df['Age'] <= 45  # this is equivelant to the line above\n",
    "df_over_45 = df[mask_over_45]\n",
    "# df_over_45 = df[df['Age'] > 45]  # this is equivelant to above.\n",
    "print(mask_over_45)\n",
    "print(df_over_45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*\n",
    "Use conditional statements to make a mask and check .value_counts() on it to see how many people:\n",
    "* Are older than 60\n",
    "* Have social security numbers starting with '4'\n",
    "* Live in Philatelphia or are named Hannah\n",
    "* Do not live in Dallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using .query to select rows\n",
    ".query lets us use a sql like syntax to select rows.  This is nice becaues it can be more readable than a conditional statement for a mask, it might be better to use a mask for cases like:\n",
    "* Your column names have special characters\n",
    "* You are generating your query/condition programatically\n",
    "* You are using operations like .str.contains or other functions in your query.\n",
    "\n",
    "Documentation and a few good examples: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html.  This shows an example with quoting a column name with a space in it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.query('Age > 30 and City == \"Chicago\"')\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting files\n",
    "If we want to output our data to a file, there are some built in tools to do this. For excel files, we need to make sure the \"openpyxl\" package is installed - this is the default engine that pandas uses to generate the file.  \n",
    "\n",
    "### Single Sheet Excel\n",
    "This is pretty simple, just using the to_excel function and giveng it a filename to write to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can comment out the pip line after running it once\n",
    "%pip install openpyxl\n",
    "out_file = 'flowers.xlsx'\n",
    "flowers.to_excel(out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Sheets Excel\n",
    "For a multi-sheet excel, we need to open a writer and write in each sheet.  We use a with statement so the writer is closed automtically after we add our sheets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'all_data.xlsx'\n",
    "sheets = {'flowers': flowers, 'fruits': fruits, 'df': df}\n",
    "with pd.ExcelWriter(out_file) as writer:\n",
    "    for sheet_name, data in sheets.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "There are many other built in options for generating output from a dataframe.  A few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "flowers.to_csv('flowers.csv')\n",
    "\n",
    "# json\n",
    "flowers.to_json('flowers.json')\n",
    "\n",
    "# html - this generates a table that can be viewed in a web browser\n",
    "flowers.to_html('flowers.html')\n",
    "\n",
    "# sql - note that you need to remove the .db file to re-run this cell\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('flowers.db')\n",
    "flowers.to_sql('flowers', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*:\n",
    "Hopefully you've found some data that you find interesting and you would like to anaylze, make graphs for, and build a note book to present.  Start a new notebook for this and import your data.   You may want to clean up column names and/or make a selection of the data to look at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
